原文：[Yin的笔记本，非常推荐](http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E5%8F%8C%E7%9B%AE%E7%9B%B8%E6%9C%BA.html#%E4%B8%89%E8%A7%92%E5%AE%9A%E4%BD%8D-triangulation)
图片部分来自[知乎，大黑](https://zhuanlan.zhihu.com/p/460119182)
本文大部分内容来自上方链接，在此基础上加入了一些自己的思考。非原创，仅作为个人笔记。
# 0. 主要内容

| 任务                | 场景数据 | 相机数据 | 输入                     | 输出               |
| ----------------- | ---- | ---- | ---------------------- | ---------------- |
| Pose Estimation   | 已知   | 测得   | 相机视野内各特征点在空间中的位置       | 相机位姿             |
| Triangulation     | 测得   | 已知   | 双目相机位姿和相机视野内特征点在图片上的位置 | 各特征点在空间中的位置      |
| Epipolar Geometry | 部分已知 | 测得   | 双目相机视野内特征点在图片上的位置      | 相机位姿和各特征点在空间中的位置 |
| Reconstruction    | 测得   | 测得   | 相机视野内2D图片              | 相机视野内各点在空间中的位置   |
首先是3D视觉四个大任务简单的介绍，

1. 姿态估计（Pose Estimation）：场景数据可以简单理解为特征点的三维坐标，例如标定物自身建立的3维坐标系下每个标定点的位置。相机数据可以理解为这些特征点在左右相机中的像素坐标。输入则需要我们进行特征点的对齐，即每个特征点的3D坐标和左右视图像素坐标的匹配。输出则是相机的位姿，方法可以使用解PnP方法，例如Opencv中的`cv::solvePnP, cv::solvePnPRansac`等。应用场景有例如增强现实（AR）：根据现实世界的 3D 点与相机图像匹配，估计相机位置；机器人导航：通过环境中的特征点估计相机（机器人）的运动轨迹。
2. 三角定位（Triangulation）：根据双目相机中相同特征点在左右视图中的位置（像素坐标），结合相机的位姿，计算这些特征点的三维位置。已知信息有同一个3D特征点投影到左右视图的像素坐标（假设已经确定了对应关系），和相机的内外参，求特征点的3D坐标。方法就是一系列三角测量的方法，例如`cv::triangulatePoints`。应用例如特征点定位等。
3. 对极几何（Epipolar Geometry）：这个范围比较广，它描述了一类几何关系，后面会有详细讲解。借用维基百科的描述:
>[**对极几何**](https://zh.wikipedia.org/zh-cn/%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95)是[立体视觉](https://zh.wikipedia.org/wiki/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89 "立体视觉")中的一种几何关系。当两个摄像机从两个不同的位置观察3D场景时，3D点及其在2D图像上的投影之间存在许多几何关系，从而导致图像点之间的约束。这些关系是基于[针孔相机模型](https://zh.wikipedia.org/wiki/%E5%B0%8F%E5%AD%94%E6%88%90%E5%83%8F "小孔成像")的假设推导出来的。

![image.png](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291458703.png?imageSlim)

4. 三维重建（Reconstruction）：通过单目或多目相机拍摄的图像重建场景中点云或表面的三维结构。例如通过 SfM（Structure from Motion）恢复相机的运动轨迹和三维结构；Nerf， 3D Gaussian等三维重建方法，是现在研究的热门方向。

# 1. 三角定位 Triangulation
Triangulation解决的是从两张图片中计算对应点的三维坐标的问题。

这个问题的假设是：

- 两张图片对应的相机内参和外参都是已知的
- 相机中对应点关系是已知的并且正确的

这样我们就可以从这两张图片中计算出指向该点方向的两条光线，进而求其交点就得这个点对应的三维坐标
![image.png|725](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291521980.png?imageSlim)

## 理想情况的三角定位
我们首先研究简化的情况，这个情况的假设就是两个相机的内参矩阵是 **完全一样** 的，而且两个相机 **在x方向是完全对齐的。**

这样我们就可以有如下的模型。

![image.png|500](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291524869.png?imageSlim)
其中b被称为baseline，代表的是两个相机的光心对应的距离。图中分别以 $C_lP$ 和 $C_rP$ 为斜边可见左右两个直角三角形，其直角边可以写出如下的关系式：
![image.png](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291531173.png?imageSlim)

![image.png](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291533115.png?imageSlim)

### 误差
![image.png](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291536540.png?imageSlim)

所以我们可以得出一下几个结论：

- 对于同一个3D点，disparity越大，误差越小
- 对于固定位置关系的两张图片，3D点离得越近误差越小

我们同时还可以总结一下baseline对他们的影响：

- b越大，triangulate的误差越小
- b越大，最近可测量距离变大

那么我们就知道怎样来提高双目相机系统的精度了：

- 增大baseline
- 增大焦距
- 尽量测量近距离的点

## 带误差的三角定位

理想情况的三角定位很好，但是在现实生活中，由于生产和装配误差，再好的双目相机，两个相机之间也不可能是完全对齐的，求出来的光线也不一定能精准相交。于是，我们需要在相没对齐光线也不能相交的情况下求解点的坐标：

![image.png|525](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291543963.png?imageSlim)
直接上截图了：
Yin的另一篇相机参数的讲解也非常推荐：[相机参数](http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E7%9B%B8%E6%9C%BA%E5%8F%82%E6%95%B0%E4%B8%8E%E5%9D%90%E6%A0%87%E7%B3%BB%E5%8F%98%E6%8D%A2.html#%E6%80%BB%E4%BD%93%E6%A6%82%E8%A7%88)
![image.png](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291552295.png?imageSlim)

![image.png](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291553356.png?imageSlim)
>[!TIP]
后续更新自己实现的triangulation方法，论文参考的是Triangulation: Why Optimize？
先贴个图，容易忘

![image.png](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291732369.png?imageSlim)

实现如下，逻辑有时间开帖单独聊

```c++
bool triangulateIDWMidpoint(const Eigen::Vector3d &x0,  
                            const Eigen::Vector3d &x1,  
                            const Eigen::Matrix3d &rotation,  
                            const Eigen::Vector3d &translation,  
                            Eigen::Ref<Eigen::Vector3d> result_point,  
                            double &error) {  
  Eigen::Vector3d rx0;  
  
  rx0 = rotation * x0;  
  
  const double p_norm = rx0.cross(x1).norm();  
  const double q_norm = rx0.cross(translation).norm();  
  const double r_norm = x1.cross(translation).norm();  
  
  const Eigen::Vector3d xprime1 =  
          (q_norm / (q_norm + r_norm)) *  
          (translation + (r_norm / p_norm) * (rx0 + x1));  
  
  result_point = rotation.transpose() * (xprime1 - translation);  
  
  const Eigen::Vector3d lambda0_rx0 = (r_norm / p_norm) * rx0;  
  const Eigen::Vector3d lambda1_x1  = (q_norm / p_norm) * x1;  
  
  error = (translation + lambda0_rx0 - lambda1_x1).norm();  
  
  /*  
  ** Eq. (9) - test adequacy  
  */  return (error * error) <  
         (std::min)((std::min)((translation + lambda0_rx0 + lambda1_x1)  
                                       .squaredNorm(),  
                               (translation - lambda0_rx0 - lambda1_x1)  
                                       .squaredNorm()),  
                    (translation - lambda0_rx0 + lambda1_x1).squaredNorm());  
}
```

# 2. 对极几何
刚才说的是知道了内参外参和点的对应关系之后，怎样进行三角定位。那往回推一步，怎样找点之间的对应关系呢？

如果直接在两张图里面找对应提取关键点进行匹配，只有在左右两边都找到的关键点才有可能匹配成功。如果对任意一点进行匹配的话，马上就会变成一个对左右两张图片所有像素的穷举搜索，复杂度为$O(hwh'w')$,其中h,w和h′,w′分别为左右两图片的长宽。

在已知相对位姿的双目相机中，能不能简化一些？ 能！现在有了相对的位姿约束，我们可以将这 **个二维的搜索问题，降低到一维。**

![image.png|400](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291738544.png?imageSlim)

如图所示，两个相机之间的相对位姿，为我们提供了一个很好的先验知识： 已知相机位姿的情况下，**从左边相机中发出的指向某个特征点的射线在右边相机中的投影可以计算出来**，所以直接在这条投影线上搜索特征点即可。

- 这个约束叫做对极约束（epipolar constraint）
- $c_l, c_r, p$组成的平面成为极平面 (epipolar plane)
- 极平面与两个图像的交线成为极线 (epipolar line)
- 所有的极线的交点我们成为极点（epipole），也是baseline和像平面的交点

![image.png|550](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291741667.png?imageSlim)

通过这个方法，我们就可以将二维搜索问题降低到一维。

![image.png](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291742484.png?imageSlim)

## 对极约束
![image.png](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291750702.png?imageSlim)
![image.png|525](https://wwwhtblog-1309008871.cos.ap-beijing.myqcloud.com/blog/202411291751867.png?imageSlim)

